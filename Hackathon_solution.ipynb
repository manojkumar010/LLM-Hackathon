{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Hackathon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "train_data = pd.read_csv('/mnt/data/train.csv')\n",
    "test_data = pd.read_csv('/mnt/data/test.csv')\n",
    "train_prompts = pd.read_csv('/mnt/data/train_prompts.csv')\n",
    "submission_example = pd.read_csv('/mnt/data/submission.csv')\n",
    "\n",
    "# Display the first few rows of each dataframe to understand their structure\n",
    "train_data_head = train_data.head()\n",
    "test_data_head = test_data.head()\n",
    "train_prompts_head = train_prompts.head()\n",
    "submission_example_head = submission_example.head()\n",
    "\n",
    "train_data_head, test_data_head, train_prompts_head, submission_example_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the train and test data with the prompts\n",
    "train_data_merged = pd.merge(train_data, train_prompts, on='prompt_id', how='left')\n",
    "test_data_merged = pd.merge(test_data, train_prompts, on='prompt_id', how='left')\n",
    "\n",
    "# Checking the first few rows of the merged data\n",
    "train_data_merged_head = train_data_merged.head()\n",
    "test_data_merged_head = test_data_merged.head()\n",
    "\n",
    "train_data_merged_head, test_data_merged_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Feature Engineering\n",
    "# 1. Text Length\n",
    "train_data_merged['text_length'] = train_data_merged['text'].apply(len)\n",
    "test_data_merged['text_length'] = test_data_merged['text'].apply(len)\n",
    "\n",
    "# 2. Word Count\n",
    "train_data_merged['word_count'] = train_data_merged['text'].apply(lambda x: len(x.split()))\n",
    "test_data_merged['word_count'] = test_data_merged['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 3. Average Word Length\n",
    "train_data_merged['avg_word_length'] = train_data_merged['text_length'] / train_data_merged['word_count']\n",
    "test_data_merged['avg_word_length'] = test_data_merged['text_length'] / test_data_merged['word_count']\n",
    "\n",
    "# Selecting features and target variable for the model\n",
    "features = ['text_length', 'word_count', 'avg_word_length']\n",
    "X = train_data_merged[features]\n",
    "y = train_data_merged['generated']\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model Building: Using a RandomForestClassifier for this task\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on the validation set\n",
    "y_pred = model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "# Evaluating the model using ROC AUC score\n",
    "roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the test dataset for prediction\n",
    "X_test = test_data_merged[features]\n",
    "\n",
    "# Making predictions on the test set\n",
    "test_predictions = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Preparing the submission file\n",
    "submission = pd.DataFrame({'id': test_data_merged['id'], 'generated': test_predictions})\n",
    "\n",
    "# Ensuring the submission format matches the example provided\n",
    "submission.head(), submission_example_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the submission file as a CSV without the index\n",
    "submission_file_path = '/mnt/data/my_submission_file.csv'\n",
    "submission.to_csv(submission_file_path, index=False)\n",
    "\n",
    "submission_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VirtualENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
